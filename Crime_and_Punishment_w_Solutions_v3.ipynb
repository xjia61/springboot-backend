{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xjia61/springboot-backend/blob/main/Crime_and_Punishment_w_Solutions_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hHq3xDQRCwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646bcec8-944e-4b28-8503-20a29683c5cd"
      },
      "source": [
        "# Run each time you're on a new Google CoLab setup or \n",
        "! pip install nltk\n",
        "! pip install bs4\n",
        "! pip install requests\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('book')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.4.1)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=44556d925a2e6d95c546e728632b10f026616adee18ca45018dac0af814cb956\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm66yOQBRCwy"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import requests"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uyY9H1TRCw1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb7a8c3-c63d-4b06-c778-83ed608d154f"
      },
      "source": [
        "text = requests.get(\"http://inta.gatech.s3.amazonaws.com/crime_and_punishment.txt\").text\n",
        "# text = requests.get(\"http://www.gutenberg.org/files/2554/2554.txt\").content\n",
        "print(text[0:100]) # Here are the first 100 characters of Project Gutenberg's crime and punishment\n",
        "print('length of text: %s characters' % len(text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ï»¿The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r\n",
            "\r\n",
            "This eBook is for th\n",
            "length of text: 1177120 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ADGjDJYRCw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c671a54-f0b2-4f11-f46a-26935d07dea6"
      },
      "source": [
        "tokens = nltk.word_tokenize(text)\n",
        "# print the number of word \"tokens\" in the book:\n",
        "print('There are %s words in crime and punishment' % len(tokens))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 253750 words in crime and punishment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uKXBpmXRCw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e26196-80bb-4146-ecc3-a79481515b12"
      },
      "source": [
        "text = nltk.Text(tokens)\n",
        "# Create an NLTK text object, in case you want to use some of the nltk.text functions\n",
        "# http://www.nltk.org/api/nltk.html#module-nltk.text \n",
        "print(text[500])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofThN0YbRCw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e43632-a6d1-477a-f888-3bcbf0f5759a"
      },
      "source": [
        "# Do word counts using the defaultdict construction in python\n",
        "# Defaultdicts are dicts where looking for a missing key gives a default value instead of an error\n",
        "# An int-based defaultdict would return a zero-integer. So starting with a defaultdict and \n",
        "# incrementing hte value every time a word occurs will give a word count\n",
        "from collections import defaultdict\n",
        "counts=defaultdict(int)\n",
        "for word in text:\n",
        "    counts[word]+=1\n",
        "print('the word \"%s\" appeared %s times' % ('the',counts['the']))\n",
        "# To sort the wordcount, do:\n",
        "sorted_counts=sorted(counts.items(), key=lambda x: -x[1])\n",
        "print('the word \"%s\" is the most common, appearing %s times' % (sorted_counts[0][0], sorted_counts[0][1]))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the word \"the\" appeared 7431 times\n",
            "the word \",\" is the most common, appearing 16176 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tExn9yWCRCxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b11938d4-f1a0-4a3d-bd48-4318afc34dd3"
      },
      "source": [
        "# An alternative way to do word counts is the \"Counter\" object\n",
        "from collections import Counter\n",
        "counts = Counter(text)\n",
        "print('the word \"%s\" appeared %s times' % ('the',counts['the']))\n",
        "# If you print \"counts\", it will print the sorted wordcount"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the word \"the\" appeared 7431 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ohllc08RCxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d74b06d-5d23-4356-90b4-09e0be61c3d2"
      },
      "source": [
        "#How many unique words are there?\n",
        "#This should be the length of the word count dictionary\n",
        "print('There are %s unique words' % len(sorted_counts))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 11247 unique words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YXaiQZKRCxH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e61b8db4-52b8-427f-919c-8a5a4bc53ad9"
      },
      "source": [
        "# To figure out how many of these words are English words, we need an english dictionary\n",
        "english=nltk.corpus.words.words('en')\n",
        "\n",
        "print(english[0:15])\n",
        "# load an english dictionary (i.e. list of english words) from nltk\n",
        "\n",
        "english = list(set([w.lower() for w in english]))\n",
        "# Then make sure all words are lowercase - so we would accept 'aaron' as a word, no matter the capitalization"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron', 'Aaronic', 'Aaronical', 'Aaronite', 'Aaronitic', 'Aaru']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnNw0RYLRCxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e31288-e922-4829-b1da-138ddf189161"
      },
      "source": [
        "es=set(english)\n",
        "ws=set(counts.keys())\n",
        "overlap = ws.intersection(es)\n",
        "missing = ws.difference(es)\n",
        "print('%s of the words in C&P are in the English dictionary, out of %s' % (len(overlap),len(ws)))\n",
        "print('A few c&p words missing from the english dictionary: %s' % str(list(missing)[0:10]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6573 of the words in C&P are in the English dictionary, out of 11247\n",
            "A few c&p words missing from the english dictionary: ['bones', 'types', 'Tea', 'adventures', 'accusing', 'II', '_buts_', 'Mercy', 'ringleaders', \"he'd\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvfSv6Y1RCxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048ac460-1be5-4fe7-f1aa-0101fd655d9e"
      },
      "source": [
        "from nltk.book import *"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rpJgvtcRCxO"
      },
      "source": [
        "from collections import Counter\n",
        "b=Counter(text)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGn7L-AzRCxQ"
      },
      "source": [
        "Below this line, we're dealing with data downloaded from a website."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr8hx11LRCxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64645410-18ed-47af-aa19-146cc047897e"
      },
      "source": [
        "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
        "url_response_object=requests.get(url)\n",
        "url_response_object.raise_for_status()\n",
        "html = url_response_object.content\n",
        "print(html[0:100]) # Here's what some of the html looks like"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tRREnKKRCxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7c2b14-f03a-4c2e-d78c-ee26c77da2f1"
      },
      "source": [
        "raw = BeautifulSoup(html).get_text()\n",
        "print(raw[0:100]) # Here's what the text looks like, after removing these html tags"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "BBC NEWS | Health | Blondes 'to die out in 200 years'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "NEWS\n",
            "  S\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foojjoYeRCxV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "f12ad659-e8df-4fe9-c468-bd4ab1dec7d6"
      },
      "source": [
        "raw[1:100]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nBBC NEWS | Health | Blondes 'to die out in 200 years'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNEWS\\n\\xa0\\xa0S\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n0yqEqCRCxX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b089f67-aaea-4bc7-d6f0-813cb19857f7"
      },
      "source": [
        "tokens = nltk.word_tokenize(raw)\n",
        "print(tokens[0:15]) # Here we've split things out by whitespace into a list of \"tokens\" or words/punctuation"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'NEWS', 'SPORT', 'WEATHER']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSLDcZdiRCxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e84a3e1-bb6e-482b-dde9-dba4a15cf2ad"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "745"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OpYV6fjRCxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e0aa835-7b59-41ea-85f2-1e21c50042c6"
      },
      "source": [
        "tokens[0:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'to\", 'die', 'out', 'in']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AcbmwI1GSjeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I1DSBMrlVGRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7Gm7IuAFVG60"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9J9y9k84VLPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m5deubf1VL22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhMmFYgjRkHF"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}